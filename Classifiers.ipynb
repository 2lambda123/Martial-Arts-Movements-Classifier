{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFM Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzk2nTE9B825",
        "colab_type": "text"
      },
      "source": [
        "# **CONSOLE**\n",
        "\n",
        "Use below cell as console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "klKMSULUvvo1",
        "colab": {}
      },
      "source": [
        "# Move all csv files into the folder movements.\n",
        "# !bash -c 'mv *.csv movements'\n",
        "\n",
        "# Delete files generated in a previous execution.\n",
        "!rm \"Accuracy.png\"\n",
        "!rm \"Loss.png\"\n",
        "!rm \"history.csv\"\n",
        "!rm \"history.json\"\n",
        "!rm \"model.h5\"\n",
        "!rm \"model.json\"\n",
        "!rm \"model_plot.png\"\n",
        "!rm \"results.txt\"\n",
        "!rm \"model_full.h5\"\n",
        "!rm \"model.tflite\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT6wQyXI7Etl",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Libraries**\n",
        "\n",
        "First, let's load all necessary libraries into the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVng0Gak7A2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Input, Activation, Masking, Conv1D, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxLmuwW37Hbu",
        "colab_type": "text"
      },
      "source": [
        "# **Parameters**\n",
        "\n",
        "Here, we are going to group the parameters of the notebook, so we can change them easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyhaLuTg7JYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path and extension of dataset files.\n",
        "dataset_path = \"movements/*.csv\"\n",
        "\n",
        "# Apply or not EWMA to smooth curves.\n",
        "ewma = True\n",
        "\n",
        "# Apply or not normalization to set data in range .\n",
        "norm = False\n",
        "\n",
        "# Value of beta in EWMA.\n",
        "ewma_b = 0.3\n",
        "\n",
        "# learning rate\n",
        "adam_learning_rate = 0.0001\n",
        "\n",
        "# Columns to drop. Options: [\"magn_field_x\", \"magn_field_y\", \"magn_field_z\",  \"accelerometer_x\", \"accelerometer_y\", \"accelerometer_z\", \"gravity_x\", \"gravity_y\", \"gravity_z\", \"gyros_x\", \"gyros_y\", \"gyros_z\", \"lin_accel_x\", \"lin_accel_y\", \"lin_accel_z\", \"game_rot_vec_x\", \"game_rot_vec_y\", \"game_rot_vec_z\"]\n",
        "drop = []\n",
        "\n",
        "# Which normalization method use: [std_mean, min_max]\n",
        "chosen_norm = \"min_max\"\n",
        "\n",
        "# Which model train. Options: [ann, rnn, cnn]\n",
        "chosen_model = \"cnn\"\n",
        "\n",
        "# Parameters of Adam optimizer:\n",
        "adam_beta_1 = 0.9\n",
        "adam_beta_2 = 0.999\n",
        "adam_decay = 0.0001\n",
        "\n",
        "# Model fit parameters:\n",
        "model_loss = 'logcosh'\n",
        "model_validation_split = 0.3\n",
        "model_batch_size = 40\n",
        "model_epochs = 4000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tGFYJQ67MaJ",
        "colab_type": "text"
      },
      "source": [
        "# **Loading the dataset**\n",
        "\n",
        "Now, we are going to load the dataset. This dataset is formed by different movements extracted from American Kenpo Karate's blocking set. Each movement is stored in a csv file and has information from the different sensors of an android device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loo3ssKn7M6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load files of dataset.\n",
        "files = glob.glob(dataset_path)\n",
        "\n",
        "# List for storing elements of the dataset.\n",
        "X_raw = []\n",
        "Y_raw = []\n",
        "\n",
        "# Read files and store each example in list.\n",
        "for path in files:\n",
        "    df = pd.read_csv(path, sep=\",\", decimal=\".\", header=0)\n",
        "    df = df.apply(pd.to_numeric)\n",
        "    df = df.drop(drop, axis=1)\n",
        "    \n",
        "    # Convert df into list.\n",
        "    X_raw.append(df.T.values.tolist())\n",
        "\n",
        "    # Get class from file name.\n",
        "    Y_raw.append(path.split(\"/\")[1][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CttB50ZG7hUt",
        "colab_type": "text"
      },
      "source": [
        "Let's print one of the movements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB4IRntm7j9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_raw[2][2])\n",
        "print(Y_raw[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IPYMDqs7nAU",
        "colab_type": "text"
      },
      "source": [
        "# **Normalization**\n",
        "\n",
        "As a way to speed up learnig, we are going to normalize the dataset. This will make each attribute to have a mean closer to 0. The normalization is done by attribute and not globally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5gG4ciw7nf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(norm):\n",
        "  X_norm = []\n",
        "  for example in X_raw:\n",
        "    resExample = []\n",
        "    for attribute in example:\n",
        "      if chosen_norm == \"std_mean\":\n",
        "        resExample.append((attribute - np.mean(example)) / np.std(attribute))\n",
        "      elif chosen_norm == \"min_max\":\n",
        "        resExample.append((attribute - np.min(example)) / (np.max(example) - np.min(example)))\n",
        "    X_norm.append(resExample.copy())\n",
        "else:\n",
        "  X_norm = X_raw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzdYbnPr7vVx",
        "colab_type": "text"
      },
      "source": [
        "Now we have the dataset normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6cT9eUW7xK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "ax1.plot(X_norm[1][2], 'tab:blue')\n",
        "ax1.set_title(\"Normalized.\")\n",
        "ax2.plot(X_raw[1][2], 'tab:orange')\n",
        "ax2.set_title(\"Raw data.\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpudqlDf7z2r",
        "colab_type": "text"
      },
      "source": [
        "# **Smoothing the curves**\n",
        "\n",
        "The curves genedated by the attributes in each sample are so sharp and noisy. We can smooth this curves using Exponentially Weighted Moving Averages. Let's define a function to smooth this curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91k9vZIP70iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def applyEWMA(beta, dataset):\n",
        "    pos = 0\n",
        "    resX = []\n",
        "    for example in dataset:\n",
        "        resExample = []\n",
        "        for attribute in example:\n",
        "            resAttribute = [attribute[0]]\n",
        "            for value in attribute:\n",
        "                resValue = (beta * value) + (1-beta) * resAttribute[pos]\n",
        "                resAttribute.append(resValue)\n",
        "                pos += 1\n",
        "            resAttribute.pop(0)\n",
        "            resExample.append(resAttribute.copy())\n",
        "            resAttribute.clear()\n",
        "            pos = 0\n",
        "        resX.append(resExample.copy())\n",
        "        resExample.clear()\n",
        "    return resX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD_xIPvi74wl",
        "colab_type": "text"
      },
      "source": [
        "Now, we can use this function to apply EWMA over the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKGH8isd76e2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(ewma):\n",
        "    X_smooth = applyEWMA(ewma_b, X_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdhwtN2W79v7",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look of how we have smoothen our curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQCuj7Br8A7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(ewma):\n",
        "  plt.plot(X_norm[1][2], label='Norm')\n",
        "  plt.plot(X_smooth[1][2], label='Smooth')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHJ00AZ3-y1q",
        "colab_type": "text"
      },
      "source": [
        "By adjusting the value of beta, we can control how smooth the new curves are. Lower values of beta gives smoother curves. If beta is 1, the generated curve is the same as the original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0IIkhFt8DaX",
        "colab_type": "text"
      },
      "source": [
        "# **Padding the sequences**\n",
        "\n",
        "For feeding the neural networks, all sequences must have the same length. Here, the length of the longest sequence is obtained, so all the sequences can be expanded (padded) till reach the length of the longest sequence in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ll9Fsm8FcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxLength = 0\n",
        "for example in X_smooth:\n",
        "  if len(example[0]) > maxLength:\n",
        "    maxLength = len(example[0])\n",
        "print(\"Length of longest sample: \" + str(maxLength))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cE8nryv8H_B",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to expand each sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s1Cwc788KG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extend each sample to maximum length\n",
        "X_expanded = []\n",
        "for example in X_smooth:\n",
        "  resExample = pad_sequences(example, padding='post', maxlen=maxLength, dtype='float64')\n",
        "  X_expanded.append(resExample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFoGOsZv8L_n",
        "colab_type": "text"
      },
      "source": [
        "Let's see an example of expanded movement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxYt81NF8N6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "ax1.plot(X_smooth[1][2], 'tab:blue')\n",
        "ax1.set_title(\"Non Expanded.\")\n",
        "ax2.plot(X_expanded[1][2], 'tab:orange')\n",
        "ax2.set_title(\"Expanded.\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRVJ_iqZ8QWt",
        "colab_type": "text"
      },
      "source": [
        "# **Convert to ndarray**\n",
        "\n",
        "Right now, we have a list of examples, where each one is a ndarray containing 18 attributes. Each attribute has the length of the previously calculated max length. We should turn that list in an ndarray, resulting in a 3-dimensional ndarray. The output Y should be formed by one-hot vectors, so we will use the function to_categorical to turn each output in a one-hot vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vsc6ss-8S1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert into Array\n",
        "X = np.asarray(X_expanded)\n",
        "print(X.shape)\n",
        "Y = np.asarray(Y_raw)\n",
        "print(Y.shape)\n",
        "Y = to_categorical(Y)\n",
        "print(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moyTL8Rw8YCP",
        "colab_type": "text"
      },
      "source": [
        "As input dataset for trainig, we are going to use the smoothen and normalized dataset X_norm. Let's get its dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNKUFhse8afz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get dimensions\n",
        "num_examples, num_attributes, num_values = X.shape\n",
        "print(X.shape)\n",
        "_, num_classes = Y.shape\n",
        "print(\"Num. Examples: \" + str(num_examples))\n",
        "print(\"Num. Attributes: \" + str(num_attributes))\n",
        "print(\"Num. Values: \" + str(num_values))\n",
        "print(\"Num. Classes: \" + str(num_classes))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZcglAIq8chK",
        "colab_type": "text"
      },
      "source": [
        "# **Create the model**\n",
        "\n",
        "Now we are going to create three models with the following characteristics: \n",
        "\n",
        "*   The first model is using a Conv1D layer. After that, dropout is applied for preventing overfitting, and the data is flattened into a one-dimensional array. Then, that array is passed to a dense layer with 6 units (number of classes), and softmax is applied.\n",
        "*   The second model is using a Dense layer. After that, dropout is applied for preventing overfitting. Then, the output of the dense layer is passed to another dense layer with 6 units (number of classes), and softmax is applied.\n",
        "*   The third model is using a LSTM layer. After that, dropout is applied for preventing overfitting, and the data is flattened into a one-dimensional array. Then, that array is passed to a dense layer with 6 units (number of classes), and softmax is applied.\n",
        "\n",
        "In the three cases below, the value for dropout is 0.5. Those are very simple models, but in the training set used with a good configuration of parameters, achieved accuracies over 0.9 for the training set and over 0.8 in the test set. LSTM is the model that got the better results with 1.0 of accuracy over the training set and 0.94 over the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_xfPn0q8exi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build CNN model.\n",
        "def cnn_model(input_shape):\n",
        "  \n",
        "    input = Input(shape=input_shape, dtype='float32')\n",
        "    i = Conv1D(filters=32, kernel_size=5, activation='relu')(input)\n",
        "    i = Dropout(0.5)(i)\n",
        "    i = Flatten()(i)\n",
        "\n",
        "    i = Dense(num_classes)(i)\n",
        "    i = Activation('softmax')(i)\n",
        "\n",
        "    model = Model(inputs=input, outputs=i)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3MW3Q4F9JIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build ANN model.\n",
        "def ann_model(input_shape):\n",
        "  \n",
        "    input = Input(shape=input_shape, dtype='float32')\n",
        "    i = Flatten()(input)\n",
        "\n",
        "    i = Dense(512)(i)\n",
        "    i = Dropout(0.5)(i)\n",
        "    i = Dense(num_classes)(i)\n",
        "    i = Activation('softmax')(i)\n",
        "\n",
        "    model = Model(inputs=input, outputs=i)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O05QPBBn83nB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build RNN model.\n",
        "def rnn_model(input_shape):\n",
        "\n",
        "    input = Input(shape=input_shape, dtype='float32')\n",
        "    #i = Masking(mask_value=0.0)(input)\n",
        "    i = LSTM(56, return_sequences=True)(input)\n",
        "    i = Dropout(0.5)(i)\n",
        "    i = Flatten()(i)\n",
        "\n",
        "    i = Dense(num_classes)(i)\n",
        "    i = Activation('softmax')(i)\n",
        "\n",
        "    model = Model(inputs=input, outputs=i)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_V2x-ad8g70",
        "colab_type": "text"
      },
      "source": [
        "Let's create the model and print the summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRn3Rlga8izD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if chosen_model == \"rnn\":\n",
        "  model = rnn_model(input_shape=(num_attributes, num_values))\n",
        "elif chosen_model == \"cnn\":\n",
        "  model = cnn_model(input_shape=(num_attributes, num_values))\n",
        "elif chosen_model == \"ann\":\n",
        "  model = ann_model(input_shape=(num_attributes, num_values))\n",
        "  \n",
        "model.summary()\n",
        "\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl_QhamH8lY9",
        "colab_type": "text"
      },
      "source": [
        "# **Training the model**\n",
        "\n",
        "Now let's train the model using an Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbUPDa-V8njP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate optimizer and compile selected model.\n",
        "opt = Adam(lr=adam_learning_rate, beta_1=adam_beta_1, beta_2=adam_beta_2, decay=adam_decay)\n",
        "model.compile(loss=model_loss, optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# Get timestamp.\n",
        "start_time = time.time()\n",
        "\n",
        "# Start training. Set verbose to 1 or 2 to print progress.\n",
        "history = model.fit(X, Y, validation_split=model_validation_split, shuffle=True, batch_size=model_batch_size, epochs=model_epochs, verbose=0)\n",
        "\n",
        "# Print accuracies and time.\n",
        "print(\"Train acc.: %s\" % history.history['accuracy'][-1])\n",
        "print(\"Test acc.: %s\" % history.history['val_accuracy'][-1])\n",
        "print(\"Time: %s\" % (time.time() - start_time))\n",
        "\n",
        "# Save accuracies and time into a file.\n",
        "f = open(\"results.txt\", \"w\")\n",
        "f.write(\"Train acc.: \" + str(history.history['accuracy'][-1]) + \"\\n\")\n",
        "f.write(\"Test acc.: \" + str(history.history['val_accuracy'][-1]) + \"\\n\")\n",
        "f.write(\"Time: \" + str(time.time() - start_time) + \"\\n\")\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zpsFed18pll",
        "colab_type": "text"
      },
      "source": [
        "# **Visualizing results**\n",
        "\n",
        "Now that we have trained our model, let's take a look to the evolution of loss and accuracy in training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioDjo98V8ru0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show accuracy in last epoch\n",
        "print(\"Train acc:\" + str(history.history['accuracy'][len(history.history['accuracy'])-1]))\n",
        "print(\"Test acc:\" + str(history.history['val_accuracy'][len(history.history['val_accuracy'])-1]))\n",
        "\n",
        "# Show data saved in history,\n",
        "print(history.history.keys())\n",
        "\n",
        "# Show accuracy and save chart into file.\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig('Accuracy.png')\n",
        "plt.show()\n",
        "\n",
        "# Show loss and save chart into file.\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig('Loss.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MChEsLS88tfD",
        "colab_type": "text"
      },
      "source": [
        "# **Saving model and weights**\n",
        "\n",
        "Let's now save the model and the weights into a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qmMprjH8u0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Save model to JSON file.\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "# Save weights to HDF5 file.\n",
        "model.save_weights(\"model.h5\")\n",
        "\n",
        "# Convert history into pandas df.\n",
        "hist_df = pd.DataFrame(history.history)\n",
        "# Save history into JSON file.\n",
        "hist_json_file = 'history.json' \n",
        "with open(hist_json_file, mode='w') as f:\n",
        "    hist_df.to_json(f)\n",
        "# Save history into CSV file.\n",
        "hist_csv_file = 'history.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "    \n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# Play a sound indicating that the training has finished.\n",
        "import IPython.display as display\n",
        "display.Audio(url=\"https://static.sfdict.com/audio/C07/C0702600.mp3\", autoplay=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfulYTWowM4X",
        "colab_type": "text"
      },
      "source": [
        "# **Loading existing model**\n",
        "An existing model can be loaded for testing over the dataset, continue training, or saving with other formats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV2_5DEDwVG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "# Clear previous Keras session.\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Load model from json file.\n",
        "with open('model.json', 'r') as f:\n",
        "    model = model_from_json(f.read())\n",
        "\n",
        "# Load weights from H5DF file.\n",
        "model.load_weights('model.h5')\n",
        "\n",
        "# If models and weights are both inside a H5DF file, load with this.\n",
        "# load_model(\"model_full.h5\")\n",
        "\n",
        "# Print summary of the model.\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8_pKHpTkJVk",
        "colab_type": "text"
      },
      "source": [
        "# **Convert the model into tflite**\n",
        "Convert the model into tflite so it can be used in android."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gREoSzrwph2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import model_from_json\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Save model and weights into a single H5DF file.\n",
        "model.save('model_full.h5')\n",
        "\n",
        "# Load the model using tf.keras.\n",
        "model = tf.keras.models.load_model('model_full.h5', compile=False)\n",
        "\n",
        "# Converting a tf.Keras model to a TensorFlow Lite model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TF Lite model.\n",
        "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}